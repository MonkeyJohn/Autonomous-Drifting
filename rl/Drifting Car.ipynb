{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gym environment.\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to update the target Q-Network toward the primary Q-network at a rate $\\tau$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the update operations to perform.\n",
    "def target_network_update_ops(tvars, tau):\n",
    "    op_holder = []\n",
    "    num_variables = len(tvars)\n",
    "    for ix, var in enumerate(tvars[0:num_variables//2]):\n",
    "        # The trainable variables for the target network is\n",
    "        # in the second half of the list of tvars.\n",
    "        updated_value = tvars[ix + num_variables//2].value() * (1 - tau) + var.value() * tau\n",
    "        op_holder.append(tvars[ix + num_variables//2].assign(updated_value))\n",
    "    return op_holder\n",
    "\n",
    "# Apply the update operations on the target network.\n",
    "def target_network_update_apply(sess, ops):\n",
    "    for op in ops:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute the Huber loss with delta = 1.\n",
    "def huber_loss(diff):\n",
    "    diff = tf.abs(diff)\n",
    "    return tf.where(tf.less(diff, 1), tf.square(diff), diff - 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cache():\n",
    "    def __init__(self, max_size=50000):\n",
    "        self.buffer = []\n",
    "        self.max_size = max_size\n",
    "    \n",
    "    def add(self, data):\n",
    "        if len(self.buffer)+ len(data) > self.max_size:\n",
    "            # Evict the oldest experiences if not enough space.\n",
    "            ix = len(self.buffer) + len(data) - self.max_size\n",
    "            self.buffer[0:ix] = [] \n",
    "        self.buffer.extend(data)\n",
    "\n",
    "class ExperienceReplayBuffer(Cache):\n",
    "    def __init__(self, max_size=50000):\n",
    "        Cache.__init__(self, max_size)\n",
    "    \n",
    "    def sample_batch(self, batch_size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer, batch_size)), [batch_size, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork():\n",
    "    def __init__(self, lr, s_size, a_size, h_size, o_size):\n",
    "        if o_size % 2 != 0:\n",
    "            raise ValueError('Number of outputs from final layer must be even')\n",
    "            \n",
    "        # Forward pass of the network.\n",
    "        # output: batch_size x s_size\n",
    "        self.state_input = tf.placeholder(shape=[None, s_size], dtype=tf.float32)\n",
    "        \n",
    "        xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "        # output: batch_size x h_size\n",
    "        layer1 = slim.fully_connected(self.state_input, h_size,\n",
    "                                     weights_initializer=xavier_init,\n",
    "                                     biases_initializer=None,\n",
    "                                     activation_fn=tf.nn.tanh)\n",
    "        # output: batch_size x o_size\n",
    "        self.fully_connected_output = slim.fully_connected(layer1, o_size,\n",
    "                                                         weights_initializer=xavier_init,\n",
    "                                                         biases_initializer=None,\n",
    "                                                         activation_fn=tf.nn.tanh)\n",
    "        \n",
    "        # Average Dueling\n",
    "        # output(each): batch_size x o_size/2\n",
    "        self.V_stream, self.A_stream = tf.split(self.fully_connected_output, num_or_size_splits=2, axis=1)\n",
    "        self.V_weights = tf.Variable(xavier_init([o_size//2, 1]))\n",
    "        self.A_weights = tf.Variable(xavier_init([o_size//2, a_size]))\n",
    "        \n",
    "        # Regularization.\n",
    "        reg = tf.reduce_sum(tf.square(self.V_weights)) + tf.reduce_sum(tf.square(self.A_weights))\n",
    "        \n",
    "        # output: batch_size x 1\n",
    "        self.state_value = tf.matmul(self.V_stream, self.V_weights)\n",
    "        # output: batch_size x a_size\n",
    "        self.advantage = tf.matmul(self.A_stream, self.A_weights)\n",
    "        \n",
    "        self.Qout = self.state_value + tf.subtract(self.advantage, \n",
    "                                                   tf.reduce_mean(self.advantage, axis=1, keep_dims=True))\n",
    "        # Predict action that maximizes Q-value.\n",
    "        self.action_predicted = tf.argmax(self.Qout, axis=1)\n",
    "        \n",
    "        # Evaluate loss and backward pass.\n",
    "        self.target_Q = tf.placeholder(shape=[None], dtype=tf.float32)             \n",
    "        self.actions_taken = tf.placeholder(shape=[None], dtype=tf.int32)       \n",
    "        self.action_taken_one_hot = tf.one_hot(self.actions_taken, a_size, dtype=tf.float32)\n",
    "        self.predicted_Q = tf.reduce_sum(tf.multiply(self.Qout, self.action_taken_one_hot), axis=1)\n",
    "        \n",
    "        self.prediction_loss = tf.reduce_mean(huber_loss(self.predicted_Q - self.target_Q)) + reg\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        self.update = self.optimizer.minimize(self.prediction_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "update_frequency = 5 # Update every 5 steps.\n",
    "gamma = 0.9 # Discount factor.\n",
    "epsilon_decay_rate = 0.9\n",
    "epsilon_min = 0.01\n",
    "annealing_steps = 10000\n",
    "total_episodes = 10000\n",
    "max_ep_length = 300\n",
    "pretrain_steps = 10000\n",
    "load_model = False\n",
    "render_env = False\n",
    "path = './model'\n",
    "h_size = 512\n",
    "o_size = 512\n",
    "a_size = env.action_space.n # Adjust this.\n",
    "s_size = env.observation_space.shape[0] # Adjust this.\n",
    "tau = 0.01\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Mean reward 50.2 epsilon : 0.0523347633027361\n",
      "Mean loss 1.49486\n",
      "Saving model...\n",
      "Mean reward 57.7 epsilon : 0.009697737297875247\n",
      "Mean loss 3.06669\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "primary_Q_network = QNetwork(learning_rate, s_size, a_size, h_size, o_size)\n",
    "target_Q_network = QNetwork(learning_rate, s_size, a_size, h_size, o_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(save_relative_paths=True)\n",
    "tvars = tf.trainable_variables()\n",
    "target_network_update_operations = target_network_update_ops(tvars, tau)\n",
    "\n",
    "experience_buffer = ExperienceReplayBuffer()\n",
    "all_rewards = []\n",
    "steps_taken = []\n",
    "all_losses = []\n",
    "\n",
    "epsilon = 1.0\n",
    "\n",
    "# Create folder to store model in, if doesn't exist.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "total_step_count = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    if load_model:\n",
    "        print('Loading latest saved model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    for episode_count in range(1, total_episodes + 1):\n",
    "        step_count = 0\n",
    "        episode_buffer = []\n",
    "        running_reward = 0\n",
    "        episode_loss = []\n",
    "        done = False\n",
    "        \n",
    "        s = env.reset()\n",
    "        while step_count < max_ep_length and not done:\n",
    "            env.render()\n",
    "            step_count += 1\n",
    "            total_step_count += 1\n",
    "            if np.random.randn(1) < epsilon or total_step_count < pretrain_steps:\n",
    "                action = np.random.randint(0, a_size)\n",
    "            else:\n",
    "                action = sess.run(primary_Q_network.action_predicted, \n",
    "                                 feed_dict={primary_Q_network.state_input:[s]})[0]\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            d_int = 1 if done else 0\n",
    "            running_reward += reward\n",
    "            episode_buffer.append([s, action, reward, next_state, d_int])\n",
    "            s = next_state\n",
    "            \n",
    "            if total_step_count > pretrain_steps:\n",
    "                if total_step_count % update_frequency == 0:\n",
    "                    train_batch = experience_buffer.sample_batch(batch_size)\n",
    "                    # Double DQN\n",
    "                    max_next_state_action =  sess.run(primary_Q_network.action_predicted,\n",
    "                             feed_dict={ primary_Q_network.state_input:np.vstack(train_batch[:,3])}) \n",
    "                    target_network_Q_values = sess.run(target_Q_network.Qout,feed_dict={\n",
    "                        target_Q_network.state_input:np.vstack(train_batch[:,3])})  \n",
    "                    \n",
    "                    Q_values_next_state = target_network_Q_values[range(batch_size),max_next_state_action]\n",
    "                    end_multiplier = (1 - train_batch[:,4])\n",
    "                    target_Q_values = train_batch[:,2] +(gamma * Q_values_next_state * end_multiplier)\n",
    "                    # Update the primary network.\n",
    "                    l, _ = sess.run([primary_Q_network.prediction_loss, primary_Q_network.update], feed_dict={\n",
    "                        primary_Q_network.state_input: np.vstack(train_batch[:,0]),\n",
    "                        primary_Q_network.actions_taken: train_batch[:,1],\n",
    "                        primary_Q_network.target_Q: target_Q_values\n",
    "                    })\n",
    "                    episode_loss.append(np.mean(l))\n",
    "                    # Update the target network.\n",
    "                    target_network_update_apply(sess, target_network_update_operations)\n",
    "\n",
    "        \n",
    "        experience_buffer.add(episode_buffer)\n",
    "        all_rewards.append(running_reward)\n",
    "        all_losses.append(np.mean(episode_loss))\n",
    "        steps_taken.append(step_count)\n",
    "        # Save model periodically.\n",
    "        if total_step_count > pretrain_steps and episode_count % 20 == 0:\n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon *= epsilon_decay_rate\n",
    "        if total_step_count > pretrain_steps and episode_count % 1000 == 0:\n",
    "            print('Saving model...')\n",
    "            saver.save(sess, path+'/model-'+str(episode_count)+'.ckpt', global_step=total_step_count)\n",
    "            print('Mean reward ' + str(np.mean(all_rewards[-10:])) + ' epsilon : ' + str(epsilon))\n",
    "            print('Mean loss ' + str(np.mean(all_losses[-10:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
